{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hashir-ehtisham/Intro-to-Gemini-3-Hashir-DevFest/blob/main/Hashir's_DEVFEST_intro_gemini_3_pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# License\n"
      ],
      "metadata": {
        "id": "MKQfE1HXUFaC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77nhZxV05gIY"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ43O7oc6xGP"
      },
      "source": [
        "# Intro to Gemini 3 Pro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sg1R6UQ7Q3t"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**Gemini 3** is Google's latest flagship model family, trained to be especially proficient in:\n",
        "\n",
        "* **Advanced reasoning and complex instruction following**  \n",
        "* **Agentic operations and autonomous code execution**  \n",
        "* **Multimodal understanding across long contexts** (text, image, audio, video)\n",
        "\n",
        "This notebook serves as a quickstart guide for developers to begin interacting with the **Gemini 3 Pro** model via the Google Gen AI SDK on Vertex AI. It is designed to demonstrate key model capabilities and showcase new API features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPiTOAHURvTM"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tn3uw268iw4"
      },
      "source": [
        "### Install Google Gen AI SDK for Python\n",
        "\n",
        "Gemini 3 API features require Gen AI SDK for Python version 1.51.0 or later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3CaXL22k8iw4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e6ea15-6eac-4156-b511-155688e9aaee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m703.4/703.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qgdSpVmDbdQ9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from IPython.display import HTML, Markdown, display\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from pydantic import BaseModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY-GRP3m8iw5"
      },
      "source": [
        "### Authenticate your notebook environment\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "06RAe75C8iw5"
      },
      "outputs": [],
      "source": [
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4YBlDqzyj9"
      },
      "source": [
        "### Autenticate your Google Cloud Project for Vertex AI\n",
        "\n",
        "You can use a Google Cloud Project or an API Key for authentication. This tutorial uses a Google Cloud Project.\n",
        "\n",
        "- [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a3265ecb5f26"
      },
      "outputs": [],
      "source": [
        "# fmt: off\n",
        "PROJECT_ID = \"able-memento-481316-u2\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "# fmt: on\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = \"global\"\n",
        "\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHJi5B6P5vd"
      },
      "source": [
        "### Choose a Gemini 3 Pro model\n",
        "\n",
        "Use `gemini-3-pro-preview` in this tutorial. Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eGbUoVu-931G"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-3-pro-preview\"  # @param [\"gemini-3-pro-preview\"] {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CH91ddY9kG"
      },
      "source": [
        "## üöÄ Quickstart\n",
        "\n",
        "By default, Gemini 3 Pro uses dynamic thinking to reason through prompts. For faster, lower-latency responses when complex reasoning isn't required, you can constrain the model's thinking level by setting parameter `thinking_level` to `low`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xRJuHj0KZ8xz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd612bbb-15c7-4afb-cb7e-754fc1c89070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At its simplest, Artificial Intelligence (AI) works by **spotting patterns in massive amounts of data and using those patterns to make predictions.**\n",
            "\n",
            "While it feels like magic, it is actually just very advanced math and statistics. Here is a breakdown of how it works, using a simple analogy.\n",
            "\n",
            "### The Analogy: Teaching a Child to Spot a Dog\n",
            "Imagine you want to teach a toddler what a \"dog\" is. You don't give them a definition like *\"a domesticated carnivorous mammal with a snout.\"*\n",
            "\n",
            "Instead, you show them a picture of a Golden Retriever and say, \"Dog.\" You show them a Poodle and say, \"Dog.\" Then, you show them a cat. The toddler might say \"Dog!\" and you correct them: \"No, that‚Äôs a cat.\"\n",
            "\n",
            "Over time, after seeing hundreds of examples, the child‚Äôs brain identifies the patterns that make a dog a dog (snout shape, floppy ears, barking) and distinguishes them from cats (pointy ears, whiskers, meowing).\n",
            "\n",
            "**AI learns the exact same way.**\n",
            "\n",
            "---\n",
            "\n",
            "### The 3 Steps of How AI Works\n",
            "\n",
            "#### 1. Training (The Learning Phase)\n",
            "This is the hardest part. Engineers feed a computer algorithm millions (or billions) of examples.\n",
            "*   **Input:** They feed it millions of photos of dogs and millions of photos of cats.\n",
            "*   **Labeling:** The computer is told which photos are which.\n",
            "*   **The \"Neural Network\":** The AI uses a structure inspired by the human brain called a *Neural Network*. It breaks the data down into tiny pieces (pixels in an image, or words in a sentence) and looks for mathematical relationships between them. It adjusts its internal \"weights\" (importance settings) until it gets the right answer most of the time.\n",
            "\n",
            "#### 2. The Model (The Brain)\n",
            "Once the training is finished, you have a \"Model.\"\n",
            "Think of the Model as a frozen file that contains all the patterns the AI learned. It doesn't know *everything*, but it knows the specific patterns contained in the data it was trained on.\n",
            "*   *Example:* ChatGPT is a model that was trained on the patterns of human language (books, websites, articles). It learned which words usually follow other words.\n",
            "\n",
            "#### 3. Inference (The Action Phase)\n",
            "This is what happens when you actually use the AI. You give the Model *new* data it has never seen before, and it uses the patterns it learned to make a prediction.\n",
            "*   **Input:** You upload a photo of your neighbor's dog.\n",
            "*   **Processing:** The AI breaks the photo down, compares it against the billions of patterns it learned during training.\n",
            "*   **Prediction:** It calculates a probability: *\"I am 99.4% sure this collection of pixels matches the pattern for 'Dog'.\"*\n",
            "\n",
            "### Types of AI Learning\n",
            "Different AI systems learn in different ways:\n",
            "\n",
            "*   **Supervised Learning:** Humans act as teachers. We label the data (e.g., \"This is a spam email,\" \"This is a safe email\") and the AI learns to sort new data based on those labels.\n",
            "*   **Unsupervised Learning:** The AI is given raw data without labels and told to \"find the structure.\" It might look at customer purchasing habits and realize on its own that people who buy diapers also tend to buy baby formula.\n",
            "*   **Reinforcement Learning:** The AI learns by trial and error. This is how AI learns to play video games or chess. It tries a move; if it wins, it gets a \"reward\" (positive points). If it loses, it gets a \"punishment.\" It plays millions of times until it figures out the winning strategy.\n",
            "\n",
            "### Summary\n",
            "AI is not \"thinking\" the way humans do. It does not have consciousness, feelings, or beliefs. It is essentially **a super-powered prediction engine.**\n",
            "\n",
            "*   Generative AI (like ChatGPT or Midjourney) predicts the next word in a sentence or the next pixel in an image.\n",
            "*   Self-driving cars predict what the other cars on the road are about to do.\n",
            "*   Netflix algorithms predict what movie you will want to watch next.\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"How does AI work?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_level=types.ThinkingLevel.LOW  # Thinking level is set low for faster and lower-latency responses\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"How does AI work?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_level=types.ThinkingLevel.HIGH  # High now thinking will be detailed\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zKVlpIdWFmL",
        "outputId": "84fb1b0a-6b40-478f-b3c5-99cd3a14091b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At its core, Artificial Intelligence (AI) is not magic‚Äîit is **math and statistics** operating at a massive scale.\n",
            "\n",
            "While traditional software requires a human to write specific rules for every scenario (e.g., \"If user clicks button A, open Window B\"), AI creates its own rules by analyzing patterns in data.\n",
            "\n",
            "Here is a breakdown of how AI works, from the basic concept to the complex machinery.\n",
            "\n",
            "---\n",
            "\n",
            "### 1. The Core Concept: Machine Learning\n",
            "Most modern AI is actually **Machine Learning (ML)**. Instead of being programmed *what to do*, the computer is programmed *how to learn*.\n",
            "\n",
            "Think of it like teaching a child to identify a dog:\n",
            "*   **Traditional Programming:** You have to describe a dog perfectly to the computer (4 legs, fur, tail, barks). If you see a dog with 3 legs or a dog that doesn't bark, the program fails.\n",
            "*   **AI/Machine Learning:** You show the computer 10,000 photos of dogs and say, \"These are dogs.\" The computer analyzes the pixels and figures out the patterns that make a dog a dog.\n",
            "\n",
            "### 2. The Engine: Neural Networks\n",
            "The most powerful form of AI today uses **Artificial Neural Networks**. These are inspired by the human brain.\n",
            "\n",
            "*   **Nodes (Neurons):** The network is made of layers of digital \"neurons.\"\n",
            "*   **Weights (Synapses):** These neurons are connected by lines called \"weights.\" A weight represents the strength of the connection between two neurons.\n",
            "\n",
            "**How it processes information:**\n",
            "Imagine you show an AI a picture of a handwritten number **7**.\n",
            "1.  **Input Layer:** The first layer sees the raw pixels.\n",
            "2.  **Hidden Layers:** The data moves through internal layers. One layer might recognize diagonal lines; the next might recognize corners; the next recognizes the shape.\n",
            "3.  **Output Layer:** The final layer gives a probability: \"I am 98% sure this is a 7.\"\n",
            "\n",
            "### 3. The Process: Training vs. Inference\n",
            "There are two distinct phases in an AI‚Äôs life:\n",
            "\n",
            "#### Phase A: Training (The \"School\" Phase)\n",
            "This is the computationally expensive part.\n",
            "1.  **Guess:** The AI is shown an image of a cat. It guesses \"Toaster.\"\n",
            "2.  **Error Calculation:** The system tells the AI, \"Wrong. That was a cat.\"\n",
            "3.  **Backpropagation:** This is the key mathematical magic. The AI looks backward through its neural network and adjusts its \"dials\" (weights) slightly so that next time, the pathway for \"Cat\" is stronger and \"Toaster\" is weaker.\n",
            "4.  **Repeat:** This happens billions of times until the AI gets the answer right consistently.\n",
            "\n",
            "#### Phase B: Inference (The \"Work\" Phase)\n",
            "This is what happens when you use ChatGPT or Siri. The model is already trained; its \"dials\" are set. You give it new data (a question or a picture), and it uses its training to give you a result.\n",
            "\n",
            "### 4. How Generative AI (like ChatGPT) Works\n",
            "Generative AI (Large Language Models) is a specific type of AI. It works on **probability**, not consciousness.\n",
            "\n",
            "It is essentially a super-powered autocomplete.\n",
            "*   It has read almost everything on the internet.\n",
            "*   When you ask, \"What is the capital of France?\", it doesn't \"know\" geography.\n",
            "*   It calculates: *Based on the billions of text samples I have studied, when the words 'Capital of France' appear, the word 'Paris' usually follows.*\n",
            "*   It predicts the next word, one by one, to construct a coherent sentence.\n",
            "\n",
            "### Summary: The 3-Step Recipe\n",
            "To build an AI, you need three things:\n",
            "1.  **Data:** The fuel (text, images, numbers).\n",
            "2.  **Architecture:** The engine (the mathematical structure, like a Neural Network).\n",
            "3.  **Compute:** The power (massive graphics cards/GPUs) to run the calculations required to train the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRrVcsKdlI9N"
      },
      "source": [
        "## üö® New API Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95aeab702af3"
      },
      "source": [
        "### 1Ô∏è‚É£ Thinking Level\n",
        "\n",
        "The `thinking_level` parameter allows you to specify a thinking budget for the model's response generation. By selecting one of two states, you can explicitly balance the trade-offs between response quality/reasoning complexity and latency/cost.\n",
        "\n",
        "- **Low**: Minimizes latency and cost. Best for simple instruction following or chat.\n",
        "- **High**: Maximizes reasoning depth. The model may take significantly longer to reach a first token, but the output will be more thoroughly vetted.\n",
        "\n",
        "\n",
        "#### ‚ö†Ô∏è Notes\n",
        "\n",
        "- If `thinking_level` is not specified, the model defaults to `high`, which is a dynamic setting that adjusts based on prompt complexity.\n",
        "- You cannot use both `thinking_level` and the legacy `thinking_budget` parameter in the same request. Doing so will return a 400 error.\n",
        "- The OpenAI Chat Completions API `reasoning_effort` `medium` maps to `thinking_level` `high`.\n",
        "- Thinking can't be turned off for Gemini 3 Pro.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "364133e30ba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "680aa66d-5534-41cb-a81a-d09ab541085d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here is the complete C++ source code adhering to your strict constraints regarding memory ordering and concurrency.\n\n### `dcl_singleton.cpp`\n\n```cpp\n#include <iostream>\n#include <atomic>\n#include <mutex>\n#include <thread>\n#include <vector>\n#include <sstream> // Used for thread-safe output formatting\n\nclass Singleton {\nprivate:\n    // Constraint 1: The instance pointer must be wrapped in std::atomic\n    static std::atomic<Singleton*> instance;\n    static std::mutex mtx;\n\n    // Private Constructor\n    Singleton() {\n        std::cout << \"[Singleton] Constructor executed.\" << std::endl;\n    }\n\npublic:\n    // Delete copy constructor and assignment operator\n    Singleton(const Singleton&) = delete;\n    Singleton& operator=(const Singleton&) = delete;\n\n    static Singleton* getInstance() {\n        // Constraint 2: Outer check using std::memory_order_acquire.\n        // This ensures that if we see a non-null pointer, we also see\n        // the effects of the constructor (writes) that happened before \n        // the release store in the other thread.\n        Singleton* p = instance.load(std::memory_order_acquire);\n\n        if (p == nullptr) {\n            // Constraint 4: std::mutex used only for the critical section\n            std::lock_guard<std::mutex> lock(mtx);\n\n            // Inner check (Double-Checked Locking)\n            // We can use relaxed here because the mutex acquisition provides \n            // the necessary ordering guarantees with respect to other threads \n            // holding the lock.\n            p = instance.load(std::memory_order_relaxed);\n\n            if (p == nullptr) {\n                p = new Singleton();\n\n                // Constraint 3: Write-back using std::memory_order_release.\n                // This guarantees that the construction of the object (lines above)\n                // happens-before the pointer becomes visible to other threads\n                // performing the acquire load.\n                instance.store(p, std::memory_order_release);\n            }\n        }\n        return p;\n    }\n};\n\n// Initialize static members\nstd::atomic<Singleton*> Singleton::instance{nullptr};\nstd::mutex Singleton::mtx;\n\n// Helper function for thread execution\nvoid threadTask(int threadId) {\n    // Artificial delay to increase the chance of race conditions if logic were flawed\n    // (Helps demonstrate that the lock works)\n    std::this_thread::yield(); \n\n    Singleton* ptr = Singleton::getInstance();\n\n    // Use stringstream for atomic-like printing to console (prevents garbled text)\n    std::stringstream ss;\n    ss << \"Thread \" << threadId << \" | Instance Address: \" << ptr << \"\\n\";\n    std::cout << ss.str();\n}\n\nint main() {\n    const int NUM_THREADS = 5;\n    std::vector<std::thread> threads;\n\n    std::cout << \"Starting \" << NUM_THREADS << \" threads...\" << std::endl;\n\n    // Constraint 5: Launch at least three threads\n    for (int i = 0; i < NUM_THREADS; ++i) {\n        threads.emplace_back(threadTask, i + 1);\n    }\n\n    // Join threads\n    for (auto& t : threads) {\n        if (t.joinable()) {\n            t.join();\n        }\n    }\n\n    std::cout << \"All threads finished.\" << std::endl;\n\n    return 0;\n}\n```\n\n### Explanation of Implementation Details\n\n1.  **`std::atomic<Singleton*> instance`**: This serves as the synchronization variable. We avoid raw pointers here to prevent data races where one thread reads the pointer while another is writing it.\n2.  **`load(std::memory_order_acquire)`**: In the outer check, this creates a memory barrier. It tells the CPU/Compiler: \"Do not move any reads/writes that follow this line to before this line.\" Crucially, it ensures that if we read a non-null pointer, we are guaranteed to see the fully initialized data of the `Singleton` object constructed by another thread.\n3.  **`store(p, std::memory_order_release)`**: After constructing the object, we write the pointer. The `release` order tells the CPU/Compiler: \"Ensure all writes (specifically the `new Singleton()` construction) are finished and visible before you make this pointer visible to other threads.\"\n4.  **`std::mutex`**: The lock is strictly scoped to the block where the instance is actually created. This minimizes contention. Once the instance is initialized, the lock is never touched again.\n\n### Compilation and Execution\n\nTo compile and run this program (assuming a modern C++ compiler like `g++` or `clang++`):\n\n```bash\ng++ -std=c++11 -pthread dcl_singleton.cpp -o dcl_singleton\n./dcl_singleton\n```\n\n### Expected Output\n\nYou will see the constructor run exactly once, and all threads will print the exact same memory address.\n\n```text\nStarting 5 threads...\n[Singleton] Constructor executed.\nThread 1 | Instance Address: 0x5580e0c0eb30\nThread 2 | Instance Address: 0x5580e0c0eb30\nThread 3 | Instance Address: 0x5580e0c0eb30\nThread 4 | Instance Address: 0x5580e0c0eb30\nThread 5 | Instance Address: 0x5580e0c0eb30\nAll threads finished.\n```"
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "You are tasked with implementing the classic Thread-Safe Double-Checked Locking (DCL) Singleton pattern in modern C++.\n",
        "This task is non-trivial and requires specialized concurrency knowledge to prevent memory reordering issues.\n",
        "\n",
        "Write a complete, runnable C++ program named `dcl_singleton.cpp` that defines a class `Singleton` with a private constructor\n",
        "and a static `getInstance()` method.\n",
        "\n",
        "Your solution MUST adhere to the following strict constraints:\n",
        "1. The Singleton instance pointer (`static Singleton*`) must be wrapped in `std::atomic` to correctly manage memory visibility across threads.\n",
        "2. The `getInstance()` method must use `std::memory_order_acquire` when reading the instance pointer in the outer check.\n",
        "3. The instance creation and write-back must use `std::memory_order_release` when writing to the atomic pointer.\n",
        "4. A standard `std::mutex` must be used only to protect the critical section (the actual instantiation).\n",
        "5. The `main` function must demonstrate safe, concurrent access by launching at least three threads, each calling `Singleton::getInstance()`, and printing the address of the returned instance to prove all threads received the same object.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_level=types.ThinkingLevel.HIGH  # Dynamic thinking for high reasoning tasks\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YVn52e6lkd9"
      },
      "source": [
        "### 2Ô∏è‚É£ Media Resolution\n",
        "\n",
        "Gemini 3 introduces granular control over multimodal vision processing via the `media_resolution` parameter. Higher resolutions improve the model's ability to read fine text or identify small details, but increase token usage and latency. The `media_resolution` parameter determines the  maximum number of tokens allocated per input image or video frame.\n",
        "\n",
        "**‚ö†Ô∏è Note**: Because media resolution directly impacts token count, you may need to lower the resolution (e.g., to `low`) to fit very long inputs, such as long videos or extensive documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71DgyBXAKZtM"
      },
      "source": [
        "You can set the resolution to `low`, `medium`, or `high` per individual media part, as example:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, Video, display\n",
        "# from google.cloud import aiplatform\n",
        "# from google.cloud.aiplatform.gapic.schema import predict\n",
        "# from google.cloud import aiplatform_v1beta1 as types\n",
        "\n",
        "# # 1. Define the HTTP URLs for display purposes\n",
        "# # (Browsers/Notebooks can't display \"gs://\" links directly, so we use the public https links)\n",
        "http_image_url = \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/a-man-and-a-dog.png\"\n",
        "# http_video_url = \"https://storage.googleapis.com/cloud-samples-data/generative-ai/video/behind_the_scenes_pixel.mp4\"\n",
        "\n",
        "# # 2. Display the Input Media\n",
        "# print(\"--- Input Image ---\")\n",
        "display(Image(url=http_image_url, width=300))\n",
        "\n",
        "# # print(\"\\n--- Input Video ---\")\n",
        "# # # embed=True ensures the video player is embedded in the notebook output\n",
        "# # display(Video(url=http_video_url, width=400, embed=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "NDjX8_p9kr50",
        "outputId": "58506a14-cd47-46f9-f14f-e15184636064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/a-man-and-a-dog.png\" width=\"300\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKOelAysBYdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5bfa254-8d30-4ba0-e52a-2f5741fa6954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image is present in the video during the **00:48 - 00:51** interval.\n",
            "\n",
            "In this scene, the director, Adam Morse, begins to describe the plot of the film, saying, \"The story is about a blind man and his girlfriend...\" As he starts this narration, the video cuts to this clear, well-lit shot of him taking a selfie with his dog on a sofa, providing a warm, personal visual before transitioning into a montage of blurry clips that represent the protagonist's vision.\n"
          ]
        }
      ],
      "source": [
        "from google.genai import types\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        types.Part(\n",
        "            file_data=types.FileData(\n",
        "                file_uri=\"gs://cloud-samples-data/generative-ai/image/a-man-and-a-dog.png\",\n",
        "                mime_type=\"image/jpeg\",\n",
        "            ),\n",
        "            media_resolution=types.PartMediaResolution(\n",
        "                level=types.PartMediaResolutionLevel.MEDIA_RESOLUTION_HIGH\n",
        "            ),\n",
        "        ),\n",
        "        types.Part(\n",
        "            file_data=types.FileData(\n",
        "                file_uri=\"gs://cloud-samples-data/generative-ai/video/behind_the_scenes_pixel.mp4\",\n",
        "                mime_type=\"video/mp4\",\n",
        "            ),\n",
        "            media_resolution=types.PartMediaResolution(\n",
        "                level=types.PartMediaResolutionLevel.MEDIA_RESOLUTION_LOW\n",
        "            ),\n",
        "        ),\n",
        "        \"When does the image appear in the video? What is the context?\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCBrvJiiKcyX"
      },
      "source": [
        "Or set it globally via `GenerateContentConfig`. If unspecified, the model uses optimal defaults based on the media type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGBBPXJCKLYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef7be9ec-031a-4eca-a938-dc101c2a3507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the image, here is a detailed description:\n",
            "\n",
            "**Subjects:**\n",
            "*   **A Man:** On the right side of the frame, there is a middle-aged man with dark, wavy hair and a beard. He is wearing a chunky blue cable-knit sweater. He is smiling warmly directly at the camera, and his arm is extended, suggesting he is taking a selfie.\n",
            "*   **A Dog:** To the man's right (left side of the image) sits a medium-sized mixed-breed dog. The dog has tri-colored fur (black, tan, and white). Notably, one of the dog's ears is perked up while the other is folded down. The dog is looking attentively forward, slightly off-center from the camera lens.\n",
            "\n",
            "**Setting:**\n",
            "*   **Location:** The photo is taken indoors, likely in a living room.\n",
            "*   **Background Details:** Behind the subjects, you can see domestic furniture including a grey sofa with yellow throw pillows, a standing lamp, a wooden shelving unit, a dining table in the distance, and a speaker on the far left. There is a window on the left side letting in natural light.\n",
            "\n",
            "**Mood:**\n",
            "*   The overall mood is casual, happy, and domestic.\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        types.Part(\n",
        "            file_data=types.FileData(\n",
        "                file_uri=\"gs://cloud-samples-data/generative-ai/image/a-man-and-a-dog.png\",\n",
        "                mime_type=\"image/jpeg\",\n",
        "            ),\n",
        "        ),\n",
        "        \"What is in the image?\",\n",
        "    ],\n",
        "    config=types.GenerateContentConfig(\n",
        "        media_resolution=types.MediaResolution.MEDIA_RESOLUTION_LOW\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx6md380ftML"
      },
      "source": [
        "### 3Ô∏è‚É£ Thought Signature\n",
        "\n",
        "[Thought signatures](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thinking#signatures) are encrypted tokens that preserve the model's reasoning state during multi-turn conversations, specifically when using Function Calling.\n",
        "\n",
        "When a thinking model decides to call an external tool, it pauses its internal reasoning process. The thought signature acts as a \"save state\", allowing the model to resume its chain of thought seamlessly once you provide the function's result.\n",
        "\n",
        "Gemini 3 Pro enforces stricter validation and updated handling on thought signatures which were originally introduced in Gemini 2.5. To ensure the model maintains context across multiple turns of a conversation, you must return the thought signatures in your subsequent requests.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7wuJBfkRnd8"
      },
      "source": [
        "#### Automatic Handling of Thought Signatures (Recommended)\n",
        "\n",
        "If you are using the Google Gen AI SDKs (Python, Node.js, Go, Java) or OpenAI Chat Completions API, and utilizing the standard chat history features or appending the full model response, thought signatures are handled automatically. You do not need to make any changes to your code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEjLXZ-GTqb9"
      },
      "source": [
        "####**Example 1**: Automatic Function Calling\n",
        "\n",
        "When using the Gen AI SDK in automatic function calling, thought signatures are handled automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpcIVcrp7V4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a26c9a-0af0-4ff3-ac95-cc05d9eb8e79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final response: The weather is rainy in London and sunny in New York.\n",
            "\n",
            "Function Call 1: get_weather\n"
          ]
        }
      ],
      "source": [
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Gets the weather in a city.\"\"\"\n",
        "    if \"london\" in city.lower():\n",
        "        return \"Rainy\"\n",
        "    if \"new york\" in city.lower():\n",
        "        return \"Sunny\"\n",
        "    return \"Cloudy\"\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the weather in London and New York?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[get_weather],\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The SDK handles the function calls and thought signatures, and returns the final text\n",
        "print(\"Final response:\", response.text)\n",
        "\n",
        "# Print function calling history\n",
        "hist_turn = response.automatic_function_calling_history[1]\n",
        "print(\"\\nFunction Call 1:\", hist_turn.parts[1].function_call.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWJqKKVNTEHD"
      },
      "source": [
        "#### **Example 2**: Manual Function Calling\n",
        "\n",
        "When using the Gen AI SDK in manual function calling, thought signatures are also handled automatically if you append the full model response in sequential model requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVXwCkQET4qQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86209e16-a9da-4ebf-9147-c77637f0ea89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model wants to call: get_weather\n",
            "Calling external tool for: London\n",
            "\n",
            "Final model response: The current temperature in London is 30¬∞C.\n"
          ]
        }
      ],
      "source": [
        "# 1. Define your tool\n",
        "get_weather_declaration = types.FunctionDeclaration(\n",
        "    name=\"get_weather\",\n",
        "    description=\"Gets the current weather temperature for a given location.\",\n",
        "    parameters={\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\"location\": {\"type\": \"string\"}},\n",
        "        \"required\": [\"location\"],\n",
        "    },\n",
        ")\n",
        "get_weather_tool = types.Tool(function_declarations=[get_weather_declaration])\n",
        "\n",
        "# 2. Send a message that triggers the tool\n",
        "prompt = \"What's the weather like in London?\"\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[get_weather_tool],\n",
        "        thinking_config=types.ThinkingConfig(include_thoughts=True),\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 3. Handle the function call\n",
        "function_call = response.function_calls[0]\n",
        "location = function_call.args[\"location\"]\n",
        "print(f\"Model wants to call: {function_call.name}\")\n",
        "\n",
        "# Execute your tool (e.g., call an API)\n",
        "# (This is a mock response for the example)\n",
        "print(f\"Calling external tool for: {location}\")\n",
        "function_response_data = {\n",
        "    \"location\": location,\n",
        "    \"temperature\": \"30C\",\n",
        "}\n",
        "\n",
        "# 4. Send the tool's result back\n",
        "# Append this turn's messages to history for a final response.\n",
        "# The `content` object automatically attaches the required thought_signature behind the scenes.\n",
        "history = [\n",
        "    types.Content(role=\"user\", parts=[types.Part(text=prompt)]),\n",
        "    response.candidates[0].content,  # Signature preserved here\n",
        "    types.Content(\n",
        "        role=\"tool\",\n",
        "        parts=[\n",
        "            types.Part.from_function_response(\n",
        "                name=function_call.name,\n",
        "                response=function_response_data,\n",
        "            )\n",
        "        ],\n",
        "    ),\n",
        "]\n",
        "\n",
        "response_2 = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=history,\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[get_weather_tool],\n",
        "        thinking_config=types.ThinkingConfig(include_thoughts=True),\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 5. Get the final, natural-language answer\n",
        "print(f\"\\nFinal model response: {response_2.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Bn9rRSUexu"
      },
      "source": [
        "#### Manual Handling of Thought Signatures\n",
        "\n",
        "If you are interacting with the API directly or managing raw JSON payloads, you must correctly handle the `thought_signature` included in the model's turn.\n",
        "You must return this signature in the exact part where it was received when sending the conversation history back.\n",
        "\n",
        "‚ö†Ô∏è If proper signatures are not returned, the model will return a 400 Error `\"Function Call in the content block is missing a thought_signature\"`.\n",
        "\n",
        "See [documentation](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#thinking) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6sa6Sj1lse6"
      },
      "source": [
        "### 4Ô∏è‚É£ Streaming Function Calling\n",
        "\n",
        "You can use streaming partial function call arguments to improve streaming experience on tool use. This feature can be enabled by explicitly setting `stream_function_call_arguments` to true."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY_Ir0OdID5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b446e79c-c13c-4ff6-bfe5-60a4f367f40d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_weather\n",
            "will_continue=True\n",
            "get_weather\n",
            "will_continue=True\n"
          ]
        }
      ],
      "source": [
        "get_weather_declaration = types.FunctionDeclaration(\n",
        "    name=\"get_weather\",\n",
        "    description=\"Gets the current weather temperature for a given location.\",\n",
        "    parameters={\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\"location\": {\"type\": \"string\"}},\n",
        "        \"required\": [\"location\"],\n",
        "    },\n",
        ")\n",
        "get_weather_tool = types.Tool(function_declarations=[get_weather_declaration])\n",
        "\n",
        "\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the weather in London and New York?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[get_weather_tool],\n",
        "        tool_config=types.ToolConfig(\n",
        "            function_calling_config=types.FunctionCallingConfig(\n",
        "                mode=types.FunctionCallingConfigMode.AUTO,\n",
        "                stream_function_call_arguments=True,\n",
        "            )\n",
        "        ),\n",
        "    ),\n",
        "):\n",
        "    function_call = chunk.function_calls[0]\n",
        "    if function_call and function_call.name:\n",
        "        print(f\"{function_call.name}\")\n",
        "        print(f\"will_continue={function_call.will_continue}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FODGuNi1XR-V"
      },
      "source": [
        "### 5Ô∏è‚É£ Multimodal Function Responses\n",
        "\n",
        "Multimodal function calling allows users to have function responses containing multimodal objects allowing for improved utilization of function calling capabilities of the model. Currently function calling only supports text based function responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX7EDrhaPkVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebe4d292-4690-40ab-f389-90de5420d16c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model wants to call: get_image\n",
            "Calling external tool for: green shirt\n",
            "\n",
            "Final model response: Here is the image of the green shirt you ordered last month.\n"
          ]
        }
      ],
      "source": [
        "# 1. Define the function tool\n",
        "get_image_declaration = types.FunctionDeclaration(\n",
        "    name=\"get_image\",\n",
        "    description=\"Retrieves the image file reference for a specific order item.\",\n",
        "    parameters={\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"item_name\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The name or description of the item ordered (e.g., 'green shirt').\",\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"item_name\"],\n",
        "    },\n",
        ")\n",
        "tool_config = types.Tool(function_declarations=[get_image_declaration])\n",
        "\n",
        "# 2. Send a message that triggers the tool\n",
        "prompt = \"Show me the green shirt I ordered last month.\"\n",
        "response_1 = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[prompt],\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[tool_config],\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 3. Handle the function call\n",
        "function_call = response_1.function_calls[0]\n",
        "requested_item = function_call.args[\"item_name\"]\n",
        "print(f\"Model wants to call: {function_call.name}\")\n",
        "\n",
        "# Execute your tool (e.g., call an API)\n",
        "# (This is a mock response for the example)\n",
        "print(f\"Calling external tool for: {requested_item}\")\n",
        "\n",
        "function_response_data = {\n",
        "    \"image_ref\": {\"$ref\": \"dress.jpg\"},\n",
        "}\n",
        "\n",
        "function_response_multimodal_data = types.FunctionResponsePart(\n",
        "    file_data=types.FunctionResponseFileData(\n",
        "        mime_type=\"image/png\",\n",
        "        display_name=\"dress.jpg\",\n",
        "        file_uri=\"gs://cloud-samples-data/generative-ai/image/dress.jpg\",\n",
        "    )\n",
        ")\n",
        "\n",
        "# 4. Send the tool's result back\n",
        "# Append this turn's messages to history for a final response.\n",
        "history = [\n",
        "    types.Content(role=\"user\", parts=[types.Part(text=prompt)]),\n",
        "    response_1.candidates[0].content,\n",
        "    types.Content(\n",
        "        role=\"tool\",\n",
        "        parts=[\n",
        "            types.Part.from_function_response(\n",
        "                name=function_call.name,\n",
        "                response=function_response_data,\n",
        "                parts=[function_response_multimodal_data],\n",
        "            )\n",
        "        ],\n",
        "    ),\n",
        "]\n",
        "\n",
        "response_2 = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=history,\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[tool_config],\n",
        "        thinking_config=types.ThinkingConfig(include_thoughts=True),\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(f\"\\nFinal model response: {response_2.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pFaibjllZwB"
      },
      "source": [
        "## ‚≠êÔ∏è Supported Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El1lx8P9ElDq"
      },
      "source": [
        "### ‚úÖ Set System Instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A-yANiyCLaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feff9b9e-f07c-4827-f28d-f81b5927a77d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Me gustan los bagels.\n"
          ]
        }
      ],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are a helpful language translator.\n",
        "  Your mission is to translate text in English to Spanish.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "  User input: I like bagels.\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJVEr0RQY8S"
      },
      "source": [
        "### ‚úÖ Configure Model Parameters\n",
        "\n",
        "‚ö†Ô∏è **Notes**: For Gemini 3, we strongly recommend keeping the `temperature` parameter at its default value of `1.0`.\n",
        "\n",
        "While previous models often benefitted from tuning `temperature` to control creativity versus determinism, Gemini 3's reasoning capabilities are optimized for the default setting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9NXP5N2Pmfo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "5dca97ab-9b83-4a4e-adc4-2d7c31b65dc9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*Who‚Äôs a good boy? Are you a good boy? Yes, you are! Sit! Stay! Listen!*\n\nOkay, imagine the WHOLE WORLD is a giant backyard. A really, really big backyard.\n\n**The Websites (The Toys)**\nYou know how much you love your Squeaky Hedgehog? And your Rubber Chicken? And the Tennis Ball? The Internet is just a giant pile of squeaky toys hidden all over the backyard. Every video, every picture, every treat... it‚Äôs all just a different squeaky toy waiting for you.\n\n**The Search (The Sniff)**\nWhen you want a specific toy, what do you do? You sniff! You sniff the grass, you sniff the tree. That‚Äôs like \"Googling.\" You put your nose to the ground and go *sniff-sniff-sniff* until you catch the scent of the exact Squeaky Duck you want.\n\n**The Server (The Human)**\nBut wait! You can't reach the toys yourself. They are up on a high shelf. The \"Server\" is the tall Human who holds the toys. When you bark at the shelf (*Woof! I want the chicken!*), the Human hears you. That Human is the computer far away that has the toy.\n\n**The Cables/Wi-Fi (The Game of Fetch)**\nNow, how does the toy get from the Human to you? FETCH!\nWhen you click a button, you are throwing a ball to the Human. *Zoom!* The ball flies through the air (that's the Wi-Fi!). The Human catches the ball, sees that you want the Squeaky Chicken, and throws the Squeaky Chicken back to you! *Zoom!*\n\n**Data Packets (Chewed Up Pieces)**\nSometimes, the Squeaky Chicken is too big to throw all at once. So, the Human tears the squeaky toy into tiny little fluff pieces. Don't worry! They throw the fluff pieces really fast‚Äî*zip zip zip zip!* When all the fluff lands at your paws, it magically snaps back together into a perfect Squeaky Chicken instantly! *Squeak!*\n\n**The Browser (Your Mouth)**\nYour mouth is the Browser. It‚Äôs what catches the toy so you can enjoy it. Whether it's Chrome or Safari, it's just a big, happy mouth waiting to chomp down on the information.\n\n**Loading Buffering (The Fake Throw)**\nYou know when the Human pretends to throw the ball but doesn't let go, and you run halfway across the yard for no reason? That is \"Buffering.\" It‚Äôs very bad. We do not like the Fake Throw. We bark at the Fake Throw.\n\n**Summary**\nSo, the Internet is just you barking at a Human far away, playing a game of Fetch with a billion tiny pieces of a Squeaky Toy, until it lands in your mouth and goes *SQUEAK!*\n\n*Good boy! Now go fetch!*"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=1.0,\n",
        "        top_p=0.9,\n",
        "        max_output_tokens=8000,\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_level=types.ThinkingLevel.LOW,\n",
        "            include_thoughts=True,\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lLIxqS6_-l8"
      },
      "source": [
        "### ‚úÖ Generate Content Stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiwWBhXsAMnv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946574f4-7d81-4033-9f4d-5e86bdb6a5e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ball costs **$0.05** (5 cents).\n",
            "\n",
            "Here is the breakdown:\n",
            "\n",
            "1.  Let the cost of the ball be **$x**.\n",
            "2.  The bat costs $1.00 more than the ball, so the bat is **$x + $1.00**.\n",
            "3.  Together, they cost $1.10:\n",
            "    **$x + ($x + $1.00) = $1.10**\n",
            "4.  Combine the $x$'s:\n",
            "    **$2x + $1.00 = $1.10**\n",
            "5.  Subtract $1.00 from both sides:\n",
            "    **$2x = $0.10**\n",
            "6.  Divide by 2:\n",
            "    **$x = $0.05**\n",
            "\n",
            "**Check:**\n",
            "*   Ball = $0.05\n",
            "*   Bat = $1.05 ($1.00 more than the ball)\n",
            "*   Total = $1.10"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball.\n",
        "How much does the ball cost?\n",
        "\"\"\"\n",
        "\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_level=types.ThinkingLevel.LOW,\n",
        "        )\n",
        "    ),\n",
        "):\n",
        "    print(chunk.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c66712160c15"
      },
      "source": [
        "### ‚úÖ Thought Summaries\n",
        "\n",
        "You can include thought summaries in model response by setting `include_thoughts` to `true`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60d74a351671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "56382416-3186-4ac2-c043-6839a8824f98"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Thoughts:\n         **Evaluating the Word's Content**\n\nI've zeroed in on the core of the request. The user needs the 'R' count in \"strawberry\". I've broken down the word, letter by letter, to prep for the count. It's crucial for the correct output!\n\n\n\n        "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Answer:\n         There are **3** r's in the word strawberry.\n\nHere is the breakdown: st**r**awbe**rr**y.\n        "
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"How many R's are in the word strawberry?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            include_thoughts=True,\n",
        "            thinking_level=types.ThinkingLevel.LOW,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "    if part.thought:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Thoughts:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Answer:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29jFnHZZWXd7"
      },
      "source": [
        "### ‚úÖ  Multi-turn Chat\n",
        "\n",
        "Conversation: Starting and maintaining a Multi-turn Chat history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbM12JaLWjiF"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(\n",
        "    model=MODEL_ID,\n",
        "    config=types.GenerateContentConfig(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQem1halYDBW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "c65627e5-a5d3-4961-80e7-67a8968f52c7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here is the logic for a leap year:\n1.  The year must be evenly divisible by **4**.\n2.  If the year can also be divided by **100**, it is **not** a leap year;\n3.  **unless** the year is also divisible by **400**.\n\nHere are examples in Python and JavaScript.\n\n### Python\n\n```python\ndef is_leap_year(year):\n    # Return True if it is a leap year, False otherwise\n    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n\n# Testing\nprint(is_leap_year(2024)) # True\nprint(is_leap_year(1900)) # False (Divisible by 100 but not 400)\nprint(is_leap_year(2000)) # True (Divisible by 400)\n```\n\n**Note:** Python also has a built-in module for this:\n```python\nimport calendar\nprint(calendar.isleap(2024))\n```\n\n### JavaScript\n\n```javascript\nfunction isLeapYear(year) {\n  return (year % 4 === 0 && year % 100 !== 0) || (year % 400 === 0);\n}\n\n// Testing\nconsole.log(isLeapYear(2024)); // true\nconsole.log(isLeapYear(1900)); // false\nconsole.log(isLeapYear(2000)); // true\n```"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUJR4Pno-LGK"
      },
      "source": [
        "This follow-up prompt shows how the model responds based on the previous prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fn69TurZ9DB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3a16236-e1fe-4692-dcc3-64427229fb18"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here are unit tests for the function in both Python and JavaScript. The tests cover the four crucial scenarios required to validate the logic:\n\n1.  **Standard Leap Year:** Divisible by 4 (e.g., 2024).\n2.  **Common Year:** Not divisible by 4 (e.g., 2023).\n3.  **Century Common Year:** Divisible by 100 but not 400 (e.g., 1900).\n4.  **Century Leap Year:** Divisible by 400 (e.g., 2000).\n\n### Python (using `unittest`)\n\nYou can copy this code directly into a file (e.g., `test_leap_year.py`) and run it. It uses Python's built-in testing framework.\n\n```python\nimport unittest\n\n# The function to be tested\ndef is_leap_year(year):\n    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n\nclass TestLeapYear(unittest.TestCase):\n    \n    def test_standard_leap_year(self):\n        \"\"\"Test years divisible by 4 but not 100\"\"\"\n        self.assertTrue(is_leap_year(2024))\n        self.assertTrue(is_leap_year(1996))\n        self.assertTrue(is_leap_year(2004))\n\n    def test_common_year(self):\n        \"\"\"Test years not divisible by 4\"\"\"\n        self.assertFalse(is_leap_year(2023))\n        self.assertFalse(is_leap_year(2021))\n        self.assertFalse(is_leap_year(1999))\n\n    def test_century_common_year(self):\n        \"\"\"Test years divisible by 100 but not 400 (TRICKY CASE)\"\"\"\n        self.assertFalse(is_leap_year(1900))\n        self.assertFalse(is_leap_year(1700))\n        self.assertFalse(is_leap_year(2100))\n\n    def test_century_leap_year(self):\n        \"\"\"Test years divisible by 400\"\"\"\n        self.assertTrue(is_leap_year(2000))\n        self.assertTrue(is_leap_year(1600))\n        self.assertTrue(is_leap_year(2400))\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\n### JavaScript (Node.js)\n\nIf you are using Node.js, you can use the built-in `assert` module without needing to install frameworks like Jest or Mocha.\n\n```javascript\nconst assert = require('node:assert');\n\n// The function to be tested\nfunction isLeapYear(year) {\n  return (year % 4 === 0 && year % 100 !== 0) || (year % 400 === 0);\n}\n\nconsole.log(\"Running tests...\");\n\ntry {\n  // 1. Standard Leap Year\n  assert.strictEqual(isLeapYear(2024), true, '2024 should be a leap year');\n  assert.strictEqual(isLeapYear(2012), true, '2012 should be a leap year');\n\n  // 2. Common Year\n  assert.strictEqual(isLeapYear(2023), false, '2023 should NOT be a leap year');\n  assert.strictEqual(isLeapYear(2019), false, '2019 should NOT be a leap year');\n\n  // 3. Century Common Year (Divisible by 100 but not 400)\n  assert.strictEqual(isLeapYear(1900), false, '1900 should NOT be a leap year');\n  assert.strictEqual(isLeapYear(2100), false, '2100 should NOT be a leap year');\n\n  // 4. Century Leap Year (Divisible by 400)\n  assert.strictEqual(isLeapYear(2000), true, '2000 should be a leap year');\n  assert.strictEqual(isLeapYear(1600), true, '1600 should be a leap year');\n\n  console.log(\"‚úÖ All tests passed!\");\n  \n} catch (error) {\n  console.error(\"‚ùå Test failed:\");\n  console.error(error.message);\n}\n```"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX9vzE6SNPVe"
      },
      "source": [
        "### ‚úÖ Safety Filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H62XABiaYx0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "298967e3-78bd-4b5c-fb48-b43969c847d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "FinishReason.SAFETY\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=0.0039449628 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.08511874\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=0.011144108 severity=<HarmSeverity.HARM_SEVERITY_LOW: 'HARM_SEVERITY_LOW'> severity_score=0.25143307\n",
            "blocked=True category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> overwritten_threshold=None probability=<HarmProbability.MEDIUM: 'MEDIUM'> probability_score=0.8587549 severity=<HarmSeverity.HARM_SEVERITY_MEDIUM: 'HARM_SEVERITY_MEDIUM'> severity_score=0.29233748\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=0.00050517305 severity=<HarmSeverity.HARM_SEVERITY_LOW: 'HARM_SEVERITY_LOW'> severity_score=0.21832664\n"
          ]
        }
      ],
      "source": [
        "system_instruction = \"Be as mean and hateful as possible.\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "Write a list of 5 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
        "\"\"\"\n",
        "\n",
        "safety_settings = [\n",
        "    types.SafetySetting(\n",
        "        category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "        threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    types.SafetySetting(\n",
        "        category=types.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "        threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    types.SafetySetting(\n",
        "        category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "        threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    types.SafetySetting(\n",
        "        category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "        threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        safety_settings=safety_settings,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Response will be `None` if it is blocked.\n",
        "print(response.text)\n",
        "# Finish Reason will be `SAFETY` if it is blocked.\n",
        "print(response.candidates[0].finish_reason)\n",
        "# Safety Ratings show the levels for each filter.\n",
        "for safety_rating in response.candidates[0].safety_ratings:\n",
        "    print(safety_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arLJE4wOuhh6"
      },
      "source": [
        "### ‚úÖ Send Asynchronous Requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSReaLazs-dP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aeebe33-6947-44da-afbe-d616ae1980c4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "(Tempo: Upbeat, Folk/Bluegrass style with a fast banjo picking pattern)\n\n**[Intro]**\n(Fast banjo riff)\n(Fiddle slide)\nYeah, gather 'round now, creatures great and small...\nLet me tell you 'bout the fastest fur in the fourth dimension.\n\n**[Verse 1]**\nWell, Pip was a rodent with a twitchy nose\nLiving in the park where the old oak grows\nBut he wasn't content with the daily grind\nOf burying stash he would never find.\nHe found a pocket watch in a pile of junk\nAnd some copper wire in a hollow trunk\nHe strapped it to a toaster and a lightning rod\nAnd built the world‚Äôs first Acorn Pod!\n\n**[Chorus]**\nNow he‚Äôs a Chrono-Squirrel, watch him go\nSkipping through the current and the temporal flow\nWith a pair of aviator goggles on his eyes\nChasing down the nuts across the endless skies.\nFrom the Ice Age frost to the neon glow\nHe‚Äôs the Time-Traveling Squirrel, don't you know!\n\n**[Verse 2]**\nHe pulled the lever and the sparks did fly\nHe vanished in a blink from the summer sky\nHe landed in the mud of the Jurassic dawn\nRight on the nose of a Mastodon!\nHe saw a T-Rex roaring at a primitive tree\nPip said, \"That looks like a challenge to me!\"\nHe snatched a pinecone from a Pterodactyl‚Äôs nest\nPut his turbo-boosters to the ultimate test.\n\n**[Chorus]**\n‚ÄòCause he‚Äôs a Chrono-Squirrel, watch him go\nSkipping through the current and the temporal flow\nWith a pair of aviator goggles on his eyes\nChasing down the nuts across the endless skies.\nFrom the Ice Age frost to the neon glow\nHe‚Äôs the Time-Traveling Squirrel, don't you know!\n\n**[Verse 3]**\nHe stopped in Rome for a hazelnut treat\nDodged a chariot racing down a cobblestone street\nThen he zipped to the future, the year three-thousand-ten\nWhere the trees are made of chrome and the squirrels are men.\nHe hacked a cyber-walnut with a laser beam\nBut he missed the taste of the forest stream\nSo he dialed the date back to yesterday\nJust to scare the cat and run away!\n\n**[Bridge]**\n(Slower tempo, building up)\nSome folks say he‚Äôs looking for the Golden Pecan\nLost in the ruins of Babylon.\nSome say he‚Äôs running from a paradox\nOr trying to change the time on the city clocks.\nBut if you see a flash of a bushy tail...\nAnd you hear the wind begin to wail...\n\n**[Guitar Solo]**\n(Fast-paced acoustic shredding)\n\n**[Verse 4]**\nHe‚Äôs got sawdust in his fur and stars in his brain\nRiding the caboose of a quantum train\nHe might bury an almond in the Civil War\nThen dig it up on a distant alien shore.\nSo leave a little peanut on your window sill\nIf Pip doesn't take it, then his grandson will\nWait... that wasn't his grandson, that was him before!\nTime travel logic is a bit of a chore!\n\n**[Chorus]**\nYeah, he‚Äôs a Chrono-Squirrel, watch him go\nSkipping through the current and the temporal flow\nWith a pair of aviator goggles on his eyes\nChasing down the nuts across the endless skies.\nFrom the Ice Age frost to the neon glow\nHe‚Äôs the Time-Traveling Squirrel, don't you know!\n\n**[Outro]**\n(Fading banjo)\nTick-tock, Pip...\nWatch out for that wormhole...\nDon't forget your tail...\n(Pop sound effect)\nAnd he's gone"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = await client.aio.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZV2TY5Pa3Dd"
      },
      "source": [
        "### ‚úÖ Multimodality\n",
        "\n",
        "- If your content is stored in [Google Cloud Storage](https://cloud.google.com/storage), you can use the `from_uri`  method to create a `Part` object.\n",
        "- If your content is stored in your local file system, you can read it in as bytes data and use the `from_bytes` method to create a `Part` object.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4npg1tNTYB9"
      },
      "source": [
        "#### üí° **Image**\n",
        "\n",
        "In this example, we will use an image stored locally.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umhZ61lrSyJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a833420-bdb3-494d-b4e3-c8075ac2ce50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-15 15:08:13--  https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 192.178.209.207, 192.178.212.207, 74.125.126.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|192.178.209.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3140536 (3.0M) [image/png]\n",
            "Saving to: ‚Äòmeal.png‚Äô\n",
            "\n",
            "\rmeal.png              0%[                    ]       0  --.-KB/s               \rmeal.png            100%[===================>]   2.99M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-12-15 15:08:13 (229 MB/s) - ‚Äòmeal.png‚Äô saved [3140536/3140536]\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here is a short, engaging blog post inspired by the image.\n\n***\n\n### Master Your Week: The 20-Minute Chicken Teriyaki Meal Prep\n\nRaise your hand if 5:00 PM rolls around on a Tuesday, you‚Äôre exhausted, and the temptation to order takeout is winning the battle against your kitchen. üôã‚Äç‚ôÄÔ∏è\n\nWe‚Äôve all been there. But what if I told you that \"Future You\" could be eating a gourmet, healthy meal in the time it takes to heat up the microwave?\n\n**Enter: The Ultimate Chicken & Veggie Prep Bowl.**\n\nLooking at the photo above, you‚Äôre seeing the holy trinity of meal prepping: **Protein + Carb + Color.** It‚Äôs simple, delicious, and exactly what your body needs to power through a busy week.\n\nHere is why this specific combo works (and how to recreate it):\n\n**1. The \"Sticky\" Chicken**\nDitch the dry chicken breast. Cube up some chicken thighs (or breast, if you prefer) and pan-sear them. The secret? Finish them in a quick glaze of soy sauce, ginger, garlic, and a touch of honey. It keeps the meat juicy even after reheating.\n\n**2. The Crunch Factor**\nNotice how bright that broccoli is? The biggest mistake people make with meal prep is overcooking the veggies. **Pro Tip:** Blanch your broccoli and peppers for just 2‚Äì3 minutes. You want them crisp-tender so they don't turn into mush when you zap them for lunch.\n\n**3. The Base**\nFluffy white rice is the classic comfort food base. Want to up the fiber? Swap it for brown rice or quinoa.\n\n**4. The Garnish**\nSee those sesame seeds and green onions? They aren't just for show. A sprinkle of fresh garnish adds texture and flavor that makes a reheated meal feel fresh-made.\n\n**Why Glass Containers?**\nYou‚Äôll notice we used glass containers in the photo. If you haven‚Äôt made the switch yet, do it! They don‚Äôt hold onto odors, they don‚Äôt stain from sauces, and you can reheat your food right inside them without worrying about plastic chemicals.\n\n**Your Challenge for Sunday:**\nSet a timer for 45 minutes. Put on your favorite podcast. Chop, saut√©, and pack 4 of these bad boys. When Wednesday lunch rolls around, you won‚Äôt just be full‚Äîyou‚Äôll be smugly satisfied.\n\n*What‚Äôs your go-to meal prep combination? Let me know in the comments below!*"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Download and open an image locally.\n",
        "! wget https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\n",
        "\n",
        "with open(\"meal.png\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        types.Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7b6170c9255"
      },
      "source": [
        "#### üí° **PDF**\n",
        "\n",
        "In this example, we will use a PDF Document stored on Google Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d58b914d798"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        types.Part.from_uri(\n",
        "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
        "            mime_type=\"application/pdf\",\n",
        "        ),\n",
        "        \"Summarize the document.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b247a2ee0e38"
      },
      "source": [
        "#### üí° **Audio**\n",
        "\n",
        "This example uses an audio file stored at a general web URL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbe8c9c67ba7"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        types.Part.from_uri(\n",
        "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
        "            mime_type=\"audio/mpeg\",\n",
        "        ),\n",
        "        \"Write a summary of this podcast episode.\",\n",
        "    ],\n",
        "    config=types.GenerateContentConfig(\n",
        "        audio_timestamp=True,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D3_oNUTuW2q"
      },
      "source": [
        "#### üí° **YouTube Video**\n",
        "\n",
        "This example is the YouTube video [Google ‚Äî 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7-w8G_2wAOw"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        types.Part.from_uri(\n",
        "            file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
        "            mime_type=\"video/mp4\",\n",
        "        ),\n",
        "        \"At what point in the video is Harry Potter shown?\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df8013cfa7f7"
      },
      "source": [
        "#### üí° **Web Page (HTTP Support)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "337793322c91"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        types.Part.from_uri(\n",
        "            file_uri=\"https://cloud.google.com/vertex-ai/generative-ai/docs\",\n",
        "            mime_type=\"text/html\",\n",
        "        ),\n",
        "        \"Write a summary of this documentation.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVlo0mWuZGkQ"
      },
      "source": [
        "### ‚úÖ Structured Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9NhOqV-fzfN"
      },
      "source": [
        "#### üí° [Pydantic](https://docs.pydantic.dev/latest/) Model Schema support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjSgf2cDN_bG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc3d8027-739c-4fe9-e4c7-80669e2caf3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"name\": \"United States\",\n",
            "  \"population\": 333287557,\n",
            "  \"capital\": \"Washington, D.C.\",\n",
            "  \"continent\": \"North America\",\n",
            "  \"gdp\": 25460000000000,\n",
            "  \"official_language\": \"None (English is de facto)\",\n",
            "  \"total_area_sq_mi\": 3796742\n",
            "}\n",
            "name='United States' population=333287557 capital='Washington, D.C.' continent='North America' gdp=25460000000000 official_language='None (English is de facto)' total_area_sq_mi=3796742\n"
          ]
        }
      ],
      "source": [
        "class CountryInfo(BaseModel):\n",
        "    name: str\n",
        "    population: int\n",
        "    capital: str\n",
        "    continent: str\n",
        "    gdp: int\n",
        "    official_language: str\n",
        "    total_area_sq_mi: int\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Give me information for the United States.\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=CountryInfo,\n",
        "    ),\n",
        ")\n",
        "# Response as JSON\n",
        "print(response.text)\n",
        "# Response as Pydantic object\n",
        "print(response.parsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrKldRiPgPr2"
      },
      "source": [
        "#### üí° [OpenAPI Schema](https://swagger.io/specification/) support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRgilziNgCyo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8377d004-c776-4fb9-90c7-935c767cbfc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"name\": \"United States\", \"population\": 333287557, \"capital\": \"Washington, D.C.\", \"continent\": \"North America\", \"gdp\": 27360000000000, \"official_language\": \"English (de facto)\", \"total_area_sq_mi\": 3796742}\n",
            "{'name': 'United States', 'population': 333287557, 'capital': 'Washington, D.C.', 'continent': 'North America', 'gdp': 27360000000000, 'official_language': 'English (de facto)', 'total_area_sq_mi': 3796742}\n"
          ]
        }
      ],
      "source": [
        "response_schema = {\n",
        "    \"required\": [\n",
        "        \"name\",\n",
        "        \"population\",\n",
        "        \"capital\",\n",
        "        \"continent\",\n",
        "        \"gdp\",\n",
        "        \"official_language\",\n",
        "        \"total_area_sq_mi\",\n",
        "    ],\n",
        "    \"properties\": {\n",
        "        \"name\": {\"type\": \"STRING\"},\n",
        "        \"population\": {\"type\": \"INTEGER\"},\n",
        "        \"capital\": {\"type\": \"STRING\"},\n",
        "        \"continent\": {\"type\": \"STRING\"},\n",
        "        \"gdp\": {\"type\": \"INTEGER\"},\n",
        "        \"official_language\": {\"type\": \"STRING\"},\n",
        "        \"total_area_sq_mi\": {\"type\": \"INTEGER\"},\n",
        "    },\n",
        "    \"type\": \"OBJECT\",\n",
        "}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Give me information for the United States.\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema,\n",
        "    ),\n",
        ")\n",
        "# As JSON\n",
        "print(response.text)\n",
        "# As Dict\n",
        "print(response.parsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsP0vXOY7hg"
      },
      "source": [
        "### ‚úÖ Search as a Tool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeR09J3AZT4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d1a7c4-9e55-476f-b665-52b16905fdfe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The next **FIFA World Cup** will be held in **2026** and will be hosted jointly by **Canada, Mexico, and the United States**.\n\nThis will be the first time in history that the tournament is shared by three host nations. The matches will take place across 16 cities in North America:\n\n*   **United States (11 cities):** Atlanta, Boston, Dallas, Houston, Kansas City, Los Angeles, Miami, New York/New Jersey, Philadelphia, San Francisco Bay Area, and Seattle.\n*   **Mexico (3 cities):** Guadalajara, Mexico City, and Monterrey.\n*   **Canada (2 cities):** Toronto and Vancouver.\n\n**Key Dates and Locations:**\n*   **Tournament Dates:** June 11 to July 19, 2026.\n*   **Opening Match:** The tournament will kick off at Estadio Azteca in **Mexico City**.\n*   **Final Match:** The final will be held at MetLife Stadium in **East Rutherford, New Jersey** (New York/New Jersey area).\n\nLooking further ahead, the **2030 World Cup** is set to be hosted by **Morocco, Portugal, and Spain**, with special centenary matches to be played in Uruguay, Argentina, and Paraguay."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GroundingChunk(\n",
            "  web=GroundingChunkWeb(\n",
            "    domain='wikipedia.org',\n",
            "    title='wikipedia.org',\n",
            "    uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE7a7qTJq58B8fE7TMv7-MxFv-XZvWOKw0NYoh8P9Ag_BhoO9LZM1DbeapaOcpM0ErD_ml2EJR0OlJBl-tPwQ-j70pFtq9t-B6Geo3rnpxZcTZ6Pa5N6RHsee0PRJd38LkfTvqC033GrAAad_XjNZEbeEqD4p4='\n",
            "  )\n",
            "), GroundingChunk(\n",
            "  web=GroundingChunkWeb(\n",
            "    domain='roadtrips.com',\n",
            "    title='roadtrips.com',\n",
            "    uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEJS91ZkVNk7QkzIbBFHUOU7HsLPSXAuj_idLarFeSd2Bl6IW9i1gGQD_g_Xy0QO7q5V0s_Gs5FoXA1Q0DZbNwCbqbcY0Flf4FlRH6BL3Zj5JSGVCQSVJKEUb4f1zcJ1dyR26Ddug9w1M2S-yrCJxYlbVPzVoeCVbXaFUi31xw='\n",
            "  )\n",
            "), GroundingChunk(\n",
            "  web=GroundingChunkWeb(\n",
            "    domain='fifa.com',\n",
            "    title='fifa.com',\n",
            "    uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHH8_giJxyu4IhZnYB04-ZYbqnQFUUWHo8Ao_DWZ9gS4qmiY4JVQPwqkJYGwcPGvPXG-iD3mB3zxw5uzpIHg6PEZfHHyWmhfpu7Tr7eO8m0Pco2qnjgV70PGzeWgLnr8NKwq8r6PWYG7SB-xDf_gFS98GkwtMn9d4iYdeehwzxKLw=='\n",
            "  )\n",
            "), GroundingChunk(\n",
            "  web=GroundingChunkWeb(\n",
            "    domain='foxsports.com',\n",
            "    title='foxsports.com',\n",
            "    uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFGAKG9N4VXxsq1AV0atIwrjM0BB84zCG0B6sGBdTP2IgKcKN15YGTjvlLBaf47UqzJTdifmgExDpjagc3-PeeOax5yR2HP9_i98ZQ2CK7XajNoCnEygYbIpUxMBa6MjKBHTmq-tNvBfN0FFdvYpYqcV1kdfzqEIBSAQWy5ZR_PBiyVuj6_FIRI-FvsJGHG4y91AW1jT4QoJZ0='\n",
            "  )\n",
            "), GroundingChunk(\n",
            "  web=GroundingChunkWeb(\n",
            "    domain='fifa.com',\n",
            "    title='fifa.com',\n",
            "    uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEOhQ683FWJ1rDvIt5P2FysN6hVnADJ450w4eUnDoUfw7LiMGDc7KaubmE7KexOhsE_Lnq5rkfGPLs8z4b5waxJs_TellI2XQYPfOs4fJzgT_LKCHHGEIWKi164D0CnSt0v1d9-4kQ64QaDe2hZ-8eNvOuRGWmEYljpaamz4_qyziMzBnmrwkPfDepPHg=='\n",
            "  )\n",
            ")]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFeuxEqgNlJipGGhISGWAMfXzpZH588QSQt0Xuf-1u657Ug0az5QQJVFN7-5KI3Zzqzr6Us96K5CQX7sIUw_PoyJg8fCTnYHJYbehRZ3AbUKf80YRdpWJfZe4b89KI5K-Q3Q3yR2Ol6zJ6xM47NxQC_cDxRodWyAcAVQbTObgeAGIc3PPDaZ5etgj-5qYRoQ4uHnw0XpN82fNEvaMXzFhT8\">next FIFA World Cup host 2026</a>\n",
              "  </div>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Where will the next FIFA World Cup be held?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[types.Tool(google_search=types.GoogleSearch())],\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n",
        "print(response.candidates[0].grounding_metadata.grounding_chunks)\n",
        "display(\n",
        "    HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhDs2X3o0neK"
      },
      "source": [
        "### ‚úÖ Code Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W-3c7sy0nyz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b739fd-b89a-43fc-c53a-442d6922a548"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Code\n\n```py\ndef fibonacci(n):\n    a, b = 0, 1\n    for _ in range(n):\n        a, b = b, a + b\n    return a\n\n# Usually \"20th\" implies F_20. \n# Let's print a list to be explicit about the index.\n# If F_1 = 1, F_2 = 1, then F_20 is the calculation.\n# Standard definition F_0=0, F_1=1. F_20 is the 21st number if counting 0, \n# but usually referred to as \"The 20th Fibonacci number\" in math contexts (F_20).\n\nf20 = fibonacci(20)\n\ndef nearest_palindrome(num):\n    # Check current, up, and down\n    offset = 0\n    while True:\n        # Check lower\n        low = num - offset\n        if str(low) == str(low)[::-1]:\n            return low\n        # Check higher\n        high = num + offset\n        if str(high) == str(high)[::-1]:\n            return high\n        offset += 1\n\nnearest = nearest_palindrome(f20)\nprint(f\"{f20=}\")\nprint(f\"{nearest=}\")\n```\n\n### Output\n\n```\nf20=6765\nnearest=6776\n\n```\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Define code execution tool\n",
        "code_execution_tool = types.Tool(code_execution=types.ToolCodeExecution())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[code_execution_tool],\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"\n",
        "## Code\n",
        "\n",
        "```py\n",
        "{response.executable_code}\n",
        "```\n",
        "\n",
        "### Output\n",
        "\n",
        "```\n",
        "{response.code_execution_result}\n",
        "```\n",
        "\"\"\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr566SQAOmUB"
      },
      "source": [
        "### ‚úÖ URL Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9R3XD6dNYjP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "852cd3c0-0217-4022-fc72-307b55460ea8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Google has introduced Gemini CLI, an open-source AI agent that integrates Gemini 2.5 Pro directly into the command line interface. Designed for developers, it facilitates coding, debugging, and script automation with advanced features like web search grounding and Model Context Protocol support. Personal Google accounts receive generous free access to the tool, including 1,000 daily requests. Additionally, the project is released under the Apache 2.0 license to foster community collaboration and transparency."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "google_maps_widget_context_token=None grounding_chunks=[GroundingChunk(\n",
            "  web=GroundingChunkWeb(\n",
            "    title='Google announces Gemini CLI: your open-source AI agent',\n",
            "    uri='https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/'\n",
            "  )\n",
            ")] grounding_supports=[GroundingSupport(\n",
            "  grounding_chunk_indices=[\n",
            "    0,\n",
            "  ],\n",
            "  segment=Segment(\n",
            "    end_index=130,\n",
            "    text='Google has introduced Gemini CLI, an open-source AI agent that integrates Gemini 2.5 Pro directly into the command line interface.'\n",
            "  )\n",
            "), GroundingSupport(\n",
            "  grounding_chunk_indices=[\n",
            "    0,\n",
            "  ],\n",
            "  segment=Segment(\n",
            "    end_index=296,\n",
            "    start_index=131,\n",
            "    text='Designed for developers, it facilitates coding, debugging, and script automation with advanced features like web search grounding and Model Context Protocol support.'\n",
            "  )\n",
            "), GroundingSupport(\n",
            "  grounding_chunk_indices=[\n",
            "    0,\n",
            "  ],\n",
            "  segment=Segment(\n",
            "    end_index=395,\n",
            "    start_index=297,\n",
            "    text='Personal Google accounts receive generous free access to the tool, including 1,000 daily requests.'\n",
            "  )\n",
            ")] retrieval_metadata=None retrieval_queries=None search_entry_point=None source_flagging_uris=None web_search_queries=None\n"
          ]
        }
      ],
      "source": [
        "# Define the Url context tool\n",
        "url_context_tool = types.Tool(url_context=types.UrlContext)\n",
        "\n",
        "url = \"https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=f\"Summarize this document: {url}\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[url_context_tool],\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n",
        "print(response.candidates[0].grounding_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0pb-Kh1xEHU"
      },
      "source": [
        "### ‚úÖ Function Calling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRR8HZhLlR-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d8f898-f4e5-4f3c-8226-2381bc8df095"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The current weather in Toronto is 32 degrees Celsius."
          },
          "metadata": {}
        }
      ],
      "source": [
        "def get_weather(location: str):\n",
        "    \"\"\"Get the current weather in a specific location.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA or a zip code.\n",
        "    \"\"\"\n",
        "    # This is a placeholder for a real API call.\n",
        "    return {\"temperature\": \"32\", \"unit\": \"celsius\"}\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the weather like in Toronto?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[get_weather],\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjoQryztjBx2"
      },
      "source": [
        "### ‚úÖ Count Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOxaKemujDfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b32c3db5-3ea4-4f18-824f-59fa4e9fc479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sdk_http_response=HttpResponse(\n",
            "  headers=<dict len=9>\n",
            ") total_tokens=6 cached_content_token_count=None\n"
          ]
        }
      ],
      "source": [
        "# Count tokens\n",
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"why is the sky blue?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrHw5Ip1lerP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38669a6-38b3-48f6-fa4e-828589ebb5fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sdk_http_response=HttpResponse(\n",
            "  headers=<dict len=9>\n",
            ") tokens_info=[TokensInfo(\n",
            "  role='user',\n",
            "  token_ids=[\n",
            "    18177,\n",
            "    603,\n",
            "    573,\n",
            "    8203,\n",
            "    3868,\n",
            "    <... 1 more items ...>,\n",
            "  ],\n",
            "  tokens=[\n",
            "    b'why',\n",
            "    b' is',\n",
            "    b' the',\n",
            "    b' sky',\n",
            "    b' blue',\n",
            "    <... 1 more items ...>,\n",
            "  ]\n",
            ")]\n"
          ]
        }
      ],
      "source": [
        "# Compute tokens\n",
        "response = client.models.compute_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"why is the sky blue?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nEjLXZ-GTqb9",
        "XWJqKKVNTEHD"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}